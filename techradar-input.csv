name,ring,quadrant,isNew,description
Four key metrics,Adopt,Techniques,FALSE,"<p>To measure software delivery performance, more and more organizations are turning to the <strong>four key metrics</strong> as defined by the <a href=""https://www.devops-research.com/"">DORA research</a> program: change lead time, deployment frequency, mean time to restore (MTTR) and change fail percentage. This research and its statistical analysis have shown a clear link between high delivery performance and these metrics; they provide a great leading indicator for how a team, or even a whole delivery organization, is doing.</p>

<p>We're still big proponents of these metrics, but we've also learned some lessons since we first started monitoring them. And we're increasingly seeing misguided measurement approaches with tools that help teams measure these metrics based purely on their continuous delivery (CD) pipelines. In particular when it comes to the stability metrics (MTTR and change fail percentage), CD pipeline data alone doesn't provide enough information to determine what a deployment failure with real user impact is. Stability metrics only make sense if they include data about real incidents that degrade service for the users.</p>

<p>And as with all metrics, we recommend to always keep in mind the ultimate intention behind a measurement and use them to reflect and learn. For example, before spending weeks to build up sophisticated dashboard tooling, consider just regularly taking the <a href=""https://www.devops-research.com/quickcheck.html"">DORA quick check</a> in team retrospectives. This gives the team the opportunity to reflect on which <a href=""https://www.devops-research.com/research.html#capabilities"">capabilities</a> they could work on to improve their metrics, which can be much more effective than overdetailed out-of-the-box tooling.</p>"
Platform engineering product teams,Adopt,Techniques,FALSE,"<p>We continue to see <strong>platform engineering product teams</strong> as a sensible default with the key insight being that they're just another <a href=""https://martinfowler.com/articles/products-over-projects.html"">product team</a>, albeit one focused on internal platform customers. Thus it is critical to have clearly defined customers and products while using the same engineering disciplines and ways of working as any other (externally focused) product team; platform teams aren't special in this regard. We strongly caution against just renaming existing internal teams ""platform teams"" while leaving ways of working and organizational structures unchanged. We're still big fans of using concepts from <a href=""https://teamtopologies.com/"">Team Topologies</a> as we think about how best to organize platform teams. We consider platform engineering product teams to be a standard approach and a significant enabler for high-performing IT.</p>"
Zero trust architecture,Adopt,Techniques,FALSE,"<p>We keep hearing about enterprises finding their security badly compromised due to an overreliance on the ""secure"" network perimeter. Once this external perimeter is breached, internal systems prove to be poorly protected with attackers quickly and easily able to deploy automated data extraction tools and ransomware attacks that all too often remain undetected for long periods. This leads us to recommend <strong>zero trust architecture</strong> (ZTA) as a now sensible default.</p>

<p>ZTA is a paradigm shift in security architecture and strategy. It’s based on the assumption that a network perimeter is no longer representative of a secure boundary and no implicit trust should be granted to users or services based solely on their physical or network location. The number of resources, tools and platforms available to implement aspects of ZTA keeps growing and includes enforcing <a href=""/radar/techniques/security-policy-as-code"">policies as code</a> based on the least privilege and as-granular-as-possible principles and continuous monitoring and automated mitigation of threats; using <a href=""/radar/techniques/service-mesh"">service mesh</a> to enforce security control application-to-service and service-to-service; implementing <a href=""/radar/techniques/binary-attestation"">binary attestation</a> to verify the origin of the binaries; and including <a href=""/radar/techniques/secure-enclaves"">secure enclaves</a> in addition to traditional encryption to enforce the three pillars of data security: in transit, at rest and in memory. For introductions to the topic, consult the <a href=""https://csrc.nist.gov/publications/detail/sp/800-207/final"">NIST ZTA</a> publication and Google's white paper on <a href=""https://cloud.google.com/security/beyondprod"">BeyondProd</a>.</p>"
CBOR/JSON bilingual protocols,Trial,Techniques,TRUE,"<p>Although it’s been around for a while, we're seeing more and more use cases where using the <a href=""http://cbor.io/"">CBOR</a> specification for data interchange makes sense — especially in environments containing multiple types of applications communicating with one another: service to service, browser to service, and so on. One thing we've found useful with <a href=""https://github.com/sirthias/borer"">Borer</a>, a Scala implementation of a CBOR encoder/decoder, is the ability for clients to negotiate content between the binary representation and plain old JSON format. It's quite useful to have a text version viewable in a browser as well as the concise binary format. We foresee <strong>CBOR/JSON bilingual protocols</strong> picking up in popularity with the continuing rise of IoT and edge computing and other situations where the environment is tightly constrained.</p>"
Data mesh,Trial,Techniques,FALSE,"<p>Increasingly, we see a mismatch between what data-driven organizations want to achieve and what the current data architectures and organizational structures allow. Organizations want to embed data-driven decision-making, machine learning and analytics into many aspects of their products and services and how they operate internally; essentially they want to augment every aspect of their operational landscape with data-driven intelligence. Yet, we still have a ways to go before we can embed analytical data, access to it and how it is managed into the business domains and operations. Today, every aspect of managing analytical data is externalized outside of the operational business domains to the data team and to the data management monoliths: data lakes and data warehouses. <strong><a href=""https://martinfowler.com/articles/data-monolith-to-mesh.html"">Data mesh</a></strong> is a decentralized sociotechnical approach to remove the dichotomy of analytical data and business operation. Its objective is to embed sharing and using analytical data into each operational business domain and close the gap between the operational and analytical planes. It's founded on four principles: domain data ownership, data as a product, self-serve data platform and computational federated governance.</p>

<p>Our teams have been implementing the <a href=""https://martinfowler.com/articles/data-mesh-principles.html"">data mesh architecture</a>; they've created new architectural abstractions such as the data product quantum to encapsulate the code, data and policy as an autonomous unit of analytical data sharing embedded into operational domains; and they've built self-serve data platform capabilities to manage the lifecycle of data product quanta in a declarative manner as described in <em><a href=""https://www.oreilly.com/library/view/data-mesh/9781492092384/"">Data Mesh</a></em>. Despite our technical advances, we're still experiencing friction using the existing technologies in a data mesh topology, not to mention the resistance of business domains to embrace sharing and using data as a first-class responsibility in some organizations.</p>"
Living documentation in legacy systems,Trial,Techniques,TRUE,"<p><a href=""https://livebook.manning.com/book/specification-by-example/chapter-3/"">Living documentation</a>, which comes from the behavior-driven development (BDD) community, is often considered a privilege for those well-maintained codebases with executable specifications. We found that this technique can also be applied to legacy systems. Lack of business knowledge is a common obstacle encountered by teams when doing system modernization. Code is usually the only trustworthy source of truth because staff turnover and existing documentation are outdated. Therefore it's very important to reestablish the association between the documentation and the code and spread the business knowledge among the team when we take over a legacy system. In practice, we would first try to go to the codebase and deepen our understanding of the business through simple cleanup and safe refactoring. During the process, we'll need to add annotations to the code so that we're able to automatically generate living documentation later. This is very different from doing BDD in green-field projects, but it's a good start in legacy systems. Based on the generated documentation, we would try to convert some of the specs into executable high-level automation tests. Do this iteratively, and eventually you could get <strong>living documentation in legacy systems</strong> that is closely associated with the code and partially executable.</p>"
Micro frontends for mobile,Trial,Techniques,FALSE,"<p>Since introducing them in the Radar in 2016, we've seen widespread adoption of <a href=""/radar/techniques/micro-frontends"">micro frontends</a> for web UIs. Recently, however, we've seen projects extend this architectural style to include <strong>micro frontends for mobile</strong> applications as well. When the application becomes sufficiently large and complex, it becomes necessary to distribute the development over multiple teams. This presents the challenge of maintaining team autonomy while integrating their work into a single app. Some teams write their own frameworks to enable this development style, and in the past we've mentioned <a href=""/radar/languages-and-frameworks/atlas-and-beehive"">Atlas and Beehive</a> as possible ways to simplify the problem of integrating multiteam app development. More recently, we've seen teams using <a href=""/radar/languages-and-frameworks/react-native"">React Native</a> to accomplish the same thing. Each React Native micro frontend is kept in its own repository where it can be built, tested and deployed separately. The team responsible for the overall application can then aggregate those micro frontends built by different teams into a single released app.</p>"
Remote mob programming,Trial,Techniques,FALSE,"<p>We continue to see many teams working and collaborating remotely; for these teams <strong>remote mob programming</strong> is a technique that is well worth trying. Remote mob programming allows teams to quickly ""mob"" around an issue or piece of code without the physical constraints of only being able to fit so many people around a pairing station. Teams can quickly collaborate on an issue or piece of code using their video conferencing tool of choice without having to connect to a big display, book a physical meeting room or find a whiteboard.</p>"
Single team remote wall,Trial,Techniques,TRUE,"<p>With the increased use of remote distributed teams, one of the things we hear people have missed having is the physical team wall. This is a single place where all the various story cards, tasks, status and progress can be displayed, acting as an information radiator and hub for the team. Often the wall was an integration point with the actual data being stored in different systems. As teams have become remote, they've had to revert to looking into the individual source systems and getting an ""at a glance"" view of a project has become very difficult. A <strong>single team remote wall</strong> is a simple technique to reintroduce the team wall virtually. While there might be some overhead in keeping this up-to-date, we feel the benefits to the team are worth it. For some teams, updating the physical wall formed part of the daily ""ceremonies"" the team did together, and the same can be done with a remote wall.</p>"
